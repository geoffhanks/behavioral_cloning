% This file was created with tikzplotlib v0.10.1.
\begin{tikzpicture}

\definecolor{darkgray176}{RGB}{176,176,176}
\definecolor{darkorange25512714}{RGB}{255,127,14}
\definecolor{lightgray204}{RGB}{204,204,204}
\definecolor{steelblue31119180}{RGB}{31,119,180}

\begin{axis}[
height=\figheight,
legend cell align={left},
legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204},
tick align=outside,
tick pos=left,
title={Training and Validation Losses per Epoch},
width=\figwidth,
x grid style={darkgray176},
xlabel={Epoch},
xmajorgrids,
xmin=-6.45, xmax=157.45,
xtick style={color=black},
y grid style={darkgray176},
ylabel={Loss (MSE)},
ymajorgrids,
ymin=-0.0115417852674776, ymax=0.440338899318896,
ytick style={color=black}
]
\addplot [semithick, steelblue31119180]
table {%
1 0.419798868201334
2 0.152301646518907
3 0.128042244993383
4 0.111337959114239
5 0.0931156938917917
6 0.0815640731806235
7 0.0756720955700254
8 0.0708624413393313
9 0.0653303821193729
10 0.0607549221181183
11 0.0558468829572415
12 0.0518727011161206
13 0.0481710480213415
14 0.0476527851481411
15 0.0430648930491411
16 0.0410045913311182
17 0.0381515400452937
18 0.0381080472453249
19 0.0380626945874745
20 0.0354512862356411
21 0.0344508155874408
22 0.0316074764148312
23 0.0309556884271242
24 0.0301340259656274
25 0.0308621226713883
26 0.0289517901424649
27 0.0279425182078275
28 0.0264993700348466
29 0.0277247276320178
30 0.0267378665882049
31 0.0255605766350935
32 0.0258446067361702
33 0.0242230429914858
34 0.0250392319272748
35 0.023758723335765
36 0.0231091331922822
37 0.0224643793298776
38 0.0228639335541291
39 0.0210247712960239
40 0.0215032771397248
41 0.0206371401235622
42 0.0204912961284212
43 0.0212182800734429
44 0.0203637840813674
45 0.0205349066022256
46 0.019378587519817
47 0.0190343947475888
48 0.0186127081884147
49 0.0193024180400715
50 0.0187448720253414
51 0.0179666887803508
52 0.0177606231693795
53 0.0182433738942734
54 0.0171463669956677
55 0.017494204454124
56 0.017804045744946
57 0.017916555402519
58 0.017814928823211
59 0.0169783929079253
60 0.0165786011919504
61 0.0164452183218919
62 0.0156597005665677
63 0.0159676335975349
64 0.0161052342544464
65 0.0153341620146549
66 0.0160289335933378
67 0.0146190275681569
68 0.0166191038367399
69 0.0149216976260199
70 0.0149529360489344
71 0.0140045149546626
72 0.0148907515507334
73 0.0143217018669208
74 0.0148628654485519
75 0.0146502157633039
76 0.0136259814810312
77 0.0140899985004663
78 0.0141802837891276
79 0.0141275774428456
80 0.0140931662261415
81 0.0132227256202878
82 0.0139523510177948
83 0.0134067693468389
84 0.0127008334494573
85 0.0127214207213452
86 0.0136411194131225
87 0.0135179825749123
88 0.0129346457584728
89 0.0129601644833373
90 0.0130411502843772
91 0.0126093028250599
92 0.0126395339483498
93 0.0127027424625222
94 0.0121922000658607
95 0.0118721187861784
96 0.0121254182160481
97 0.012186102475301
98 0.0130454076524395
99 0.0115155870010027
100 0.0117465974120937
101 0.0124585188917449
102 0.011225325946542
103 0.0113634066773485
104 0.0110662424355093
105 0.0114642950483962
106 0.0116481377360159
107 0.0114736705097176
108 0.011115306443412
109 0.011225845708849
110 0.0114067603981377
111 0.0110305855195853
112 0.0106340241892044
113 0.011147216403504
114 0.0115813158611081
115 0.0115974295322674
116 0.0106727414528842
117 0.0110685630250338
118 0.00998218559070403
119 0.010785613229668
120 0.00998222254544979
121 0.0112267887500077
122 0.0105857607982249
123 0.0105314368791923
124 0.0096119851045918
125 0.00922899233005333
126 0.00990494541383887
127 0.0100465411922227
128 0.0112050003344215
129 0.0106568199306575
130 0.0110343363754701
131 0.0104710207978482
132 0.00989546725186977
133 0.00997447269093201
134 0.00941154220235273
135 0.00940403559164767
136 0.00970547565658816
137 0.00951500994441428
138 0.00899824585008482
139 0.0105598379614399
140 0.0098405513031368
141 0.00926380529854563
142 0.00962184707196407
143 0.00944365114051786
144 0.00943673796487226
145 0.00971254406997161
146 0.0100174101155555
147 0.00980781331575728
148 0.00922731544188172
149 0.0097965170317765
150 0.00969169183736957
};
\addlegendentry{Training Loss}
\addplot [semithick, darkorange25512714]
table {%
1 0.141959533733981
2 0.100362234296543
3 0.0994917043883886
4 0.074501566136522
5 0.105022752125348
6 0.0777470225335232
7 0.0581329523718783
8 0.0475350956698614
9 0.0580755269953183
10 0.076480788311788
11 0.0439816349252526
12 0.0462328548343586
13 0.0286471895640716
14 0.0391005053317973
15 0.0311092718570892
16 0.0286718063322561
17 0.0367662990731852
18 0.0346460862683931
19 0.0267238392228527
20 0.0294823900330812
21 0.0247871646757371
22 0.0242191318688648
23 0.0288449310664354
24 0.0257724639793326
25 0.0275298940283912
26 0.0251007247982281
27 0.0254831511244577
28 0.022665872120498
29 0.0225978743517771
30 0.0234954765016612
31 0.0275720508353386
32 0.0245019596568974
33 0.0257120277383365
34 0.0201836793517162
35 0.0206391950836405
36 0.0203588578251324
37 0.0231158870272338
38 0.0189889953804335
39 0.0199140401473934
40 0.0188759108223686
41 0.0201574720691756
42 0.0211311317913766
43 0.0200074339252231
44 0.0193092313861208
45 0.0210406684483002
46 0.024543952404721
47 0.0189948906024386
48 0.0210534131320726
49 0.0204826960656127
50 0.0207723440868514
51 0.0193812012655794
52 0.0165422002951215
53 0.0222685013781302
54 0.0208408541262283
55 0.0189819929663957
56 0.0170470313975654
57 0.0176756999954315
58 0.016308265373144
59 0.0173646204689118
60 0.0202616004173511
61 0.0215178044097099
62 0.0202374017680995
63 0.0166291328679238
64 0.0175533682939463
65 0.0168495493807963
66 0.0176415929629002
67 0.0170012699672952
68 0.0167995159015326
69 0.0197488712063724
70 0.0177991919202863
71 0.0164799421826111
72 0.016346976728112
73 0.0238923704625839
74 0.0172710364809193
75 0.0193575323501136
76 0.0155801365085478
77 0.016216555794069
78 0.0181081237288059
79 0.016858445483792
80 0.0223419602112179
81 0.0171715102241641
82 0.0181401288269886
83 0.0176827016078667
84 0.0150585100029795
85 0.0129503598145675
86 0.01599335167557
87 0.0139802587534567
88 0.0215080325433519
89 0.0178936014018421
90 0.0194906951597659
91 0.0175778863377803
92 0.0186691981124958
93 0.0179539777338505
94 0.0152986338054429
95 0.016786390017452
96 0.0146298960499865
97 0.0197274418980149
98 0.013866120630077
99 0.0187651601403819
100 0.0142911827649056
101 0.015722070260173
102 0.0161177579629501
103 0.0179012405619557
104 0.0146213280402922
105 0.016381476031633
106 0.0164242700083248
107 0.0175675673900904
108 0.0186032833539814
109 0.0174916297288811
110 0.016337306033321
111 0.0199928131553211
112 0.0175291840918362
113 0.0155415610826042
114 0.0173921002407691
115 0.0179655586153136
116 0.0145327035757613
117 0.0170102689648047
118 0.0159100560005754
119 0.0170949484703929
120 0.0135348937856699
121 0.0180024019592176
122 0.0153173933620565
123 0.0146565495503767
124 0.0163283051727506
125 0.0181472625284057
126 0.0178833517058852
127 0.0164813514773933
128 0.0143843409106401
129 0.0179596351912811
130 0.0176463190216704
131 0.0140913721494144
132 0.0147729717407076
133 0.014758890133401
134 0.0144268634275899
135 0.0167687398988554
136 0.0133192068053177
137 0.0192361367774928
138 0.0161888990904637
139 0.0182563816276213
140 0.0172026605769393
141 0.0166675693351343
142 0.0157558611531775
143 0.0166617776421065
144 0.0208899832447059
145 0.0159753272430472
146 0.0161439669193766
147 0.0150188416517007
148 0.0149918920165094
149 0.0154264520853758
150 0.0165713223287769
};
\addlegendentry{Validation Loss}
\end{axis}

\end{tikzpicture}
